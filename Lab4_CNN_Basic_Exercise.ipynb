{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lab4_CNN_Basic_Exercise.ipynb","version":"0.3.2","provenance":[{"file_id":"1Px3bd1jd4AjrevcIfnmH_u5Q5HGaPuGz","timestamp":1554215308301}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DPQffhRI7AG_","colab_type":"text"},"cell_type":"markdown","source":["# Lab 4.1 Basic Convolution Neural Network"]},{"metadata":{"id":"zQOUCbZKYHgH","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Copyright (c) 2019 Oscar PANG (oscarpang@vtc.edu.hk) All rights reserved.\n","\n","The MIT License\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy of \n","this software and associated documentation files (the \"Software\"), to deal in \n","the Software without restriction, including without limitation the rights to \n","use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\n","the Software, and to permit persons to whom the Software is furnished to do so,\n","subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all \n","copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\n","FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR \n","COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER \n","IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n","CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n","'''"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qUONc-x5wrJa","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.1 Import necessary packages"]},{"metadata":{"id":"Nva44BEYTfaM","colab_type":"code","colab":{}},"cell_type":"code","source":["import os, sys\n","import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pyplot\n","from glob import glob"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cIzgCE9zux-B","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.2 Upload dataset\n","\n","In this exercise, we will use the \"Plant Seedlings Dataset\" from the ***Computer Vision and Biosystems Signal Processing Group, Aarhus University, Denmark***. More information on the dataset can be found in this link: \n","\n","https://vision.eng.au.dk/plant-seedlings-dataset/\n","\n","You should have uploaded the dataset into your Google Drive in advance.\n","\n","The following code snippet will authorize access to your Google Drive and retrieve the dataset file. Execute the cell below and follow the instruction to get and enter the access code for your google drive. Upon successful, the google drive wil be mounted to the colab as **/content/gdrive**."]},{"metadata":{"id":"-pfPJezAhB0x","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gBGrnDCWvXOc","colab_type":"text"},"cell_type":"markdown","source":["You can execute some basic linux terminal commands in the cell below.\n","\n","Execute the command below in the cell for checking if you have the dataset file ready in your Google Drive.\n","\n","Run the unzip command to unzip the dataset, which contains two folders namely **train** and **test**. They respectively store the ***labelled*** image data for training and ***unlabelled*** image data for testing. This will take a while to run."]},{"metadata":{"id":"aY5yUJG_ulF-","colab_type":"code","colab":{}},"cell_type":"code","source":["ls /content/gdrive/My\\ Drive/public/plant_seedlings_dataset.zip "],"execution_count":0,"outputs":[]},{"metadata":{"id":"iwa5LFHfjCfN","colab_type":"code","colab":{}},"cell_type":"code","source":["!unzip /content/gdrive/My\\ Drive/public/plant*.zip "],"execution_count":0,"outputs":[]},{"metadata":{"id":"cSw2XuwGwAWI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Run the following command to browse the \"plant_seedlings_dataset\" directory. \n","# You should see two folders \"test\" and \"train\"\n","ls plant_seedlings_dataset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"feAr7N_xwSBC","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.3 Define Constants\n","\n","The dataset should be ready for our use. Now we have to define some environment variables and constants for our use."]},{"metadata":{"id":"TayfD14HThRI","colab_type":"code","colab":{}},"cell_type":"code","source":["# define some constants and path names\n","label_path = \"plant_seedlings_dataset/train\"\n","train_path = \"plant_seedlings_dataset/train/*/*.png\"\n","test_path = \"plant_seedlings_dataset/test/*.png\"\n","\n","IMG_SIZE = (100, 100)\n","CHANNEL = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YuH6g37fxVKH","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.4 Prepare and load the dataset\n","\n","First let's load the filenames of the plant seedling images and get the class names of the labels. The names of the seedlings are saved and indexed in a dictionary *label_dict*.\n","\n"]},{"metadata":{"id":"WgRvc0oSUZRa","colab_type":"code","colab":{}},"cell_type":"code","source":["# use glob to read the path of each image file\n","train_filenames = glob(train_path)\n","test_filenames = glob(test_path)\n","\n","#print(train_filenames)\n","#print(test_filenames)\n","\n","label_dict = {}\n","class_num = 0\n","\n","# create a dictionary for the key-value pairs of seed names and the respective values\n","for subdir in sorted(os.listdir(label_path)):\n","    label_dict[subdir] = class_num\n","    class_num+=1\n","\n","print(label_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sYISnzprx_VE","colab_type":"text"},"cell_type":"markdown","source":["Then, we read the image data (train data and test data) and the respective labels into the lists. This may take a while."]},{"metadata":{"id":"W7cE2qHlvWA0","colab_type":"code","colab":{}},"cell_type":"code","source":["train_img = []\n","train_label = []\n","\n","for filename in train_filenames:\n","    image = cv2.resize(cv2.imread(filename), IMG_SIZE)\n","    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","    seed_name = filename.split(\"/\")[2]\n","    train_img.append(image)\n","    train_label.append(label_dict[seed_name])\n","\n","# convert the lists into numpy array\n","train_img = np.asarray(train_img)\n","train_label = np.asarray(train_label)\n","\n","# examine the datasets\n","print(train_img.shape)\n","print(train_label.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yIyhSmYDyLNh","colab_type":"code","colab":{}},"cell_type":"code","source":["test_img = []\n","test_label = []\n","\n","for filename in test_filenames:\n","    image = cv2.resize(cv2.imread(filename), IMG_SIZE)\n","    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","    test_img.append(image)\n","\n","# convert the image data list into numpy array\n","test_img = np.asarray(test_img)\n","\n","# examine the datasets\n","print(test_img.shape)\n","#print(train_label.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lq3EwLwiyXrl","colab_type":"text"},"cell_type":"markdown","source":["We can visualize the image data read from the train data lists using *matplotlib*."]},{"metadata":{"id":"PRDi90Auybf0","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.rcParams['figure.figsize'] = (10.0,8.0)\n","for i in range(20):\n","    plt.subplot(4,5,i+1)\n","    plt.imshow(train_img[i])\n","\n","print(train_label[:10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b-frOav4ylC5","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.5 Data and Feature Processing\n","\n","Color image has pixel range of 0 - 255. But neural network prefers input data in the range of 0 - 1 (why?)\n","\n","###Exercise: Complete the code below to rescale the image data in the numpy array from the range of 0-255 to 0-1.\n","\n","We also encode the labels of the images into **one-hot vector**. One-hot encoding means using binary format (0 & 1) to represent the label data.\n","\n","If you have 5 classes of data, namely A, B, C, D and E. One-hot representation of a label A is then '1 0 0 0 0', label C is ''0 0 1 0 0', and so on.\n"]},{"metadata":{"id":"hLzO1T1nyp18","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import np_utils\n","\n","# rescale the pixel value of the color images from 0-255 to 0-1\n","\n","### TO DO: rescale the train image data and test image data by 255 (2 lines)\n","# ========================= ++Your code here ===========================#\n","train_img_scaled = None\n","test_img_scaled = None\n","# ======================================================================#\n","\n","# perform one-hot encoding to the train labels\n","train_label_onehot = np_utils.to_categorical(train_label)\n","\n","print(train_label_onehot[:20])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WMUYn42Ny3kW","colab_type":"text"},"cell_type":"markdown","source":["The images in the train data folder are read into the numpy array sequentially. We have to shuffle them before feeding into the CNN for training. This ensures that the data is randomly distributed and no bias will be on a particular class of data.\n","\n","During training of the model, we constantly check the accuracy and loss by taking a small sample of training data for comparing the prediction of the model under training and the respective ground truths. This help us keep track on the accuracy of the model as training progresses and check if the model is overfitting / underfitting. Such small subset of data taken from the training data is commonly known as \"validation data / set\".\n","\n","Hence, it is useful to further split the train data into two subsets, one is for the training (accounting for 80% of the total train data) and the remaining for validation during training. Note that the neural network model will not be trained / learn from the validation data.\n","\n","There are some existing libraries useful for achieving the tasks above, e.g. **sklearn preprocessing**. Here we demonstrate the skills using **numpy**.\n","\n","---\n","### Exercise: Use numpy slicing to divide the train image dataset and one-hot encoding into training and validation subsets"]},{"metadata":{"id":"MHitLJw2y07T","colab_type":"code","colab":{}},"cell_type":"code","source":["# shuffle the image dataset and split it into training dataset (80%) and validation dataset (20%)\n","np.random.seed(5)\n","indices = np.random.permutation(train_img_scaled.shape[0])\n","\n","train_img_scaled = train_img_scaled[indices]\n","train_label_onehot = train_label_onehot[indices]\n","\n","train_num = int(train_img_scaled.shape[0] * 0.8)\n","\n","# split train and evaluation sets using numpy slicing\n","x_Train = train_img_scaled[:train_num]\n","y_Train = train_label_onehot[:train_num]\n","\n","\n","### TO DO: Use numpy slicing to slice the remaining data for x_Eval and y_Eval\n","# ========================= ++Your code here ===========================#\n","x_Eval = None \n","y_Eval = None \n","# ======================================================================#\n","\n","print(x_Train.shape)\n","print(y_Train.shape)\n","print(x_Eval.shape)\n","print(y_Eval.shape)\n","print(\"Total number of samples in the train folder: \",train_img_scaled.shape[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qIyCmViAS6a-","colab_type":"text"},"cell_type":"markdown","source":["### Exercise: Employ other image processing techniques on pre-processing of image data\n","\n","The following code cell serves as a place holder for you to try different image processing techniques for enhancing the image data / reduce the size of the image data without scarificing image quality. The smaller the size of the image data, the faster the processing. This part is left for your free trial afterward."]},{"metadata":{"id":"DtSmWmbCTTVE","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO: Free trail of image processing / feature engineering techniques\n","### You may consider using OpenCV and various photo enhancement techniques to boost the accuracy\n","### Remember to save the enhanced data into the respective numpy container.\n","\n","# E.g. Try converting the color image into gray-scale\n","#x_Train = (x_Trainl[:,:,0] + x_Train[:,:,1] + x_Train[:,:,2])/3\n","#x_Eval = (x_Eval[:,:,0] + x_Eval[:,:,1] + x_Eval[:,:,2])/3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-lB2VPjmy_TC","colab_type":"text"},"cell_type":"markdown","source":["##4.1.6 Define Helper Function\n","\n","We define a graph plotting function here which can visualize the accuracy of the model on the training dataset and validation dataset throughout the training process."]},{"metadata":{"id":"gJ-yW2HAzBrn","colab_type":"code","colab":{}},"cell_type":"code","source":["# create helper function\n","def show_train_history(train_history, train, validation):\n","    plt.plot(train_history.history[train])\n","    plt.plot(train_history.history[validation])\n","    plt.title('Train History')\n","    plt.xlabel('Epochs')\n","    plt.ylabel(train)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hEPxQde4zC0j","colab_type":"text"},"cell_type":"markdown","source":["##4.1.7 Build a CNN Model\n","\n","Finally comes to the neural network model building part!\n","\n","Keras has already implemented some handy APIs for building various types of deep neural networks. It's like making a cake by stacking up different layers. You are encouraged to check out the online documentation of Kears for details of different parameters.\n","\n","https://keras.io/\n","\n","### Step a: Import Keras Packages"]},{"metadata":{"id":"gIjWKVYJzGD6","colab_type":"code","colab":{}},"cell_type":"code","source":["# import keras packages\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n","from keras import optimizers\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import TensorBoard"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E1nYFJ7SzLFq","colab_type":"text"},"cell_type":"markdown","source":["###Step b: Define Model\n","\n","We will build a Convolution Neural Network having this architecture:\n","\n","`Input -> [CONV -> RELU -> MAX_ POOL] * 3 -> FC -> RELU -> FC -> SOFTMAX`\n","\n","The model is [sequential](https://keras.io/getting-started/sequential-model-guide/) in nature. We can stack up different layers linerly using this syntax: `model.add()`\n","\n","The image data will be fed to the first 2D convolution layer with the following specifications:\n","\n","*Convolution Block 1*\n","* `Conv2D`: Input dimension: 100x100x3, No. of Filters: 32, Filter Size: 3x3, Activation Function: ReLU, Default Stride and Padding\n","* `MaxPooling2D`: Filter Size 2x2\n","---\n","**Exercise: Implement Convolution Block 2 and 3**\n","Continue to implement model using the convolution block 2 and 3 according to the requirements below. \n","\n","**Convolution Block 2**\n","* `Conv2D`: No. of filters: 64, Filter Size: 3x3, Activation Function: ReLU, Default Stride and Padding\n","* `MaxPooling2D`: Filter Size 2x2\n","\n","*Convolution Block 3*\n","* `Conv2D`: No. of filters: 128, Filter Size: 3x3, Activation Function: ReLU, Default Stride and Padding\n","* `MaxPooling2D`: Filter Size 2x2\n","\n","After Convolution Block 3, the 2D output of the convolution block will be \"roll out\" / **flatten** into a 1D vector for feeding into a fully-connected network (called **Dense** layer in Keras). Continue to implement and add the following layers to the model:\n"," \n"," *Fully Connected Layer 1*\n"," * `Dense`: Input node: 256, Activation Function: ReLU\n"," \n"," *Fully Connected Layer 2 (Output Layer)*\n"," * `Dense`: Input node: 12, Activation Function: softmax\n","\n","The final output layer will consist of 12 nodes, each of which output the probability of prediction of each seedling class after the special activation function called **softmax**. The total sum of probabilities of all classes should be equal to 1.0 in the output layer.\n","\n","---\n","\n","Refer to the Keras API Documentations for the following:\n","\n","* [Conv2D](https://keras.io/layers/convolutional/)\n","* [MaxPooling2D](https://keras.io/layers/pooling/)\n","* [Fully Connected Layer (Dense)](https://keras.io/layers/core/)\n","\n","\n","You can come back to tune various hyper-parameters in the layers and re-run the training later on."]},{"metadata":{"id":"sQvTuJL-zJRE","colab_type":"code","colab":{}},"cell_type":"code","source":["# create a sequential model\n","model = Sequential()\n","\n","# start building convolution layer\n","model.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (IMG_SIZE[0], IMG_SIZE[1], CHANNEL)))\n","model.add(MaxPooling2D((2, 2)))\n","\n","### TO DO: Implement Convolution Block 2 and 3 (~ 4 lines)\n","#================ Your code below ================#\n","\n","\n","#=================================================#\n","model.add(Flatten())\n","\n","### TO DO: Implement the 2 Fully Connected Layer (~2 lines)\n","#================ Your code below ================#\n","\n","\n","#=================================================#\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pijn-lmpFOT9","colab_type":"text"},"cell_type":"markdown","source":["You can examine the architecture of the model using `model.summary()`"]},{"metadata":{"id":"6a4P_GbIFWXi","colab_type":"code","colab":{}},"cell_type":"code","source":["model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cFeA3PyeDCLI","colab_type":"text"},"cell_type":"markdown","source":["### Step c: Define Loss Function and Parameters of Training\n","\n","After defining the model architecture, we have to compile the model with configuration of how the model is trained. "]},{"metadata":{"id":"cbEgo2B0DTKz","colab_type":"code","colab":{}},"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=1e-4), metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yIlXoT_fzbY9","colab_type":"text"},"cell_type":"markdown","source":["## 4.1.8 Model Training\n","\n","After building the model, we can start the training of the model.\n","\n","During the training, we can monitor the intermediate result of the training and can determine if the loss function can converge. If not, we can stop the training and change the parameters. This saves time especially when the model is large and trainng typically takes time.\n","\n","### Exercise: Implement the training\n","Implemnet the training using `model.fit()` the following parameters:\n","* Input Data: x_Train\n","* Input Label: y_Train\n","* Epochs (no. of iterations): 20\n","* Batch size: 128\n","* Validation data: (x_Eval, y_Eval)\n","\n","You can refer to the Keras API for [model.fit()](https://keras.io/models/model/)\n","\n","---\n","\n","When you are ready, execute the cell and you will see the training progresses from epoch 0. \n","\n","Monitor the **val acc** and Note that the loss function should be plateau and the accuracy of the model should increase as the training continues. Also notice the number of epochs when the accuracy of validation saturates.\n","\n","**The training takes roughly 30min for 20 epochs. Do not stop the Colab during training or you risks starting all over again!**"]},{"metadata":{"id":"4jgyEOVrzeHH","colab_type":"code","colab":{}},"cell_type":"code","source":["# log the start time of the training\n","start_time = time.time()\n","train_history = model.fit(x_Train, y_Train, epochs=40, batch_size=128, validation_data=(x_Eval, y_Eval))\n","\n","### TO DO: Implement the training using model.fit() and save the train history return object in train_history (1 line) \n","#================ Your code below ================#\n","train_history = None\n","#=================================================#\n","\n","# log the end time of the training\n","end_time = time.time()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HodSefwIMCGI","colab_type":"text"},"cell_type":"markdown","source":["##4.1.9 Model Evaluation\n","\n","After the training is stopped, we can visualise the results of the training by plotting the graphs of the loss function and accuracy over time."]},{"metadata":{"id":"qKbwvzgQ0UOS","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Total time elapsed = %d sec\" % int(end_time - start_time))\n","print(\"=\"*70)\n","print()\n","# evaluate model accuracy\n","scores = model.evaluate(x_Eval, y_Eval)\n","print(\"model scores = \", scores)\n","\n","# for accuracy\n","show_train_history(train_history, 'acc', 'val_acc')\n","\n","# for loss\n","show_train_history(train_history, 'loss', 'val_loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"37U4-Bou9HWK","colab_type":"text"},"cell_type":"markdown","source":["With a simple CNN having 3 convolution layers, you have built a model achieving 70% of accuracy. This is not bad though! \n","\n","But this is not the end of this Lab! We should strive for a better accuracy. Now save the model for later use and move on to the next part."]},{"metadata":{"id":"pd9kUheK7fvm","colab_type":"text"},"cell_type":"markdown","source":["## Save Trained Model\n","\n","Save the model weights into a hdf5 file format in Keras. The weights can be inputted into another model having the same model architecture and can continue to train / do neural network inferencing."]},{"metadata":{"id":"RdN8nzIh7iVl","colab_type":"code","colab":{}},"cell_type":"code","source":["filename = str(time.strftime(\"%Y%m%d_%H%M%S\"))\n","filename = 'seedling_classifier_' + filename + '.h5'\n","model.save(filename)\n","print(\"Model has been saved successfully!\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cQK8bjUq8OHY","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download the saved model to your local system\n","from google.colab import files\n","\n","files.download(filename)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0iPOwv8jOVcv","colab_type":"text"},"cell_type":"markdown","source":["#Lab4.2 Model Fine Tuning\n","\n","## 4.2.1 Fine-tuning Model\n","\n","You can return to the model architecture part and tune the hyper-parameters of the convolutional layers. Feel free to practice trail and error to see how the hyper-parameters affect the training time and accuracy.\n","\n","\n","## 4.2.2 Building a Deeper Network\n","\n","Usually a deeper (more layers), more complex neural network perform better than a simple, shallow neural network. \n","\n","In the above example, you have implemented the model with 3 convolution blocks. You can try to insert more convolution blocks before the fully connected layer, or even combining two convolutional layer into a single build block like the following:\n","\n","`Input -> [CONV -> RELU -> CONV -> RELU -> POOL] * 3 -> FC -> RELU -> FC -> SOFTMAX`\n","\n","This may increase training time significantly, however, as more parameters have to be trained in the model.\n","\n","\n","\n"]},{"metadata":{"id":"cPVUH26TY_0R","colab_type":"text"},"cell_type":"markdown","source":["#Lab 4.3 Data Augmentation\n","\n","We have built a CNN using roughly 3000 images of 12 classes of seedlings with accuracy of 70%. Each class of seedling has about 200 - 300 images, which is not quite enough for building a robust image classifier. In fact, this scenario happens commonly in real life, that you are always presented with insufficient data for tackling an image recognition tasks using machine learning approach.\n","\n","In this section, we learn to use a useful trick called **Data Augmentation** which boosts the quantity of data for trainng and helps increase the performance of the model.\n","\n","---\n","\n","\n"]},{"metadata":{"id":"CLd9B0mL2tDQ","colab_type":"code","colab":{}},"cell_type":"code","source":["### TO DO: uncomment the parameters in the ImageDataGenerator for trying various combination of image augmentation methods\n","### We first try flipping horizontally and vertically the images to produce some new images\n","\n","# define the way new training data is generated\n","train_datagen = ImageDataGenerator(#featurewise_center=True, \n","                             #featurewise_std_normalization=True,\n","                             #rotation_range=90,\n","                             #zoom_range=0.2,\n","                             #shear_range=0.2,\n","                             #width_shift_range=0.2,\n","                             #height_shift_range=0.2,\n","                             horizontal_flip=True,\n","                             vertical_flip=True) \n","                             #zca_whitening=True)\n","  \n","# create the data generator by feeding the original data from numpy array into the generator\n","train_generator = train_datagen.flow(x = x_Train, y = y_Train, batch_size = 128)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"N-Pu9X3OSwRd","colab_type":"text"},"cell_type":"markdown","source":["To train the model with new data generation, we will use model.fit_generator().\n","\n","Consult the [API](https://keras.io/preprocessing/image/) on the usage and example.\n","\n","You can also create a data generator for validation dataset and feed it into the model training too."]},{"metadata":{"id":"RczO5EgqStKP","colab_type":"code","colab":{}},"cell_type":"code","source":["start_time = time.time()\n","\n","train_history = model.fit_generator(train_generator, \n","                                    steps_per_epoch=len(x_Train), epochs = 10, \n","                                    validation_data=(x_Eval, y_Eval))\n","\n","end_time = time.time()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CDeQzEucXFmT","colab_type":"text"},"cell_type":"markdown","source":["Likewise, evaluate the accuracy of the model."]},{"metadata":{"id":"xZUp20GZXLR0","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Total time elapsed = %d sec\" % int(end_time - start_time))\n","print(\"=\"*70)\n","print()\n","# evaluate model accuracy\n","scores = model.evaluate(x_Eval, y_Eval)\n","print(\"model scores = \", scores)\n","\n","# for accuracy\n","show_train_history(train_history, 'acc', 'val_acc')\n","\n","# for loss\n","show_train_history(train_history, 'loss', 'val_loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"twf9Zio8XPno","colab_type":"code","colab":{}},"cell_type":"code","source":["# save model weights for future use\n","filename = str(time.strftime(\"%Y%m%d_%H%M%S\"))\n","filename = 'seedling_classifier_data_augmentation_' + filename + '.h5'\n","model.save(filename)\n","print(\"Model has been saved successfully!\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pByqgM0wXV-k","colab_type":"code","colab":{}},"cell_type":"code","source":["# Download the saved model to your local system\n","from google.colab import files\n","\n","files.download(filename)"],"execution_count":0,"outputs":[]}]}
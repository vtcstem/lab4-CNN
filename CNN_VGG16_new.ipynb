{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CNN_VGG16.ipynb","version":"0.3.2","provenance":[{"file_id":"1o2ybMywlhWth5bdeVmL9jLiiFxyTjqBh","timestamp":1554344565912},{"file_id":"1Px3bd1jd4AjrevcIfnmH_u5Q5HGaPuGz","timestamp":1554112580127}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DPQffhRI7AG_","colab_type":"text"},"cell_type":"markdown","source":["# Lab 4.4 CNN Using VGG-16\n","\n","In this exercise we will borrow the pre-trained weights and convolutional layers from the famous VGG16 network to our model. VGG16 has been trained on some millions of images of daily objects. The weights of such should be well tuned against various features and abstract representation of the objects. They will be useful for recognizing images of plants and seedlings as well.\n","\n","This technique is sometimes referred as \"Transferred Learning\" in some readings.\n","\n","---\n","\n","To start this lab 4.4, remember to restart the runtime of your Colab to avoid running out of resources in the runtime."]},{"metadata":{"id":"qUONc-x5wrJa","colab_type":"text"},"cell_type":"markdown","source":["## Import necessary packages\n","\n","Let's import some packages first. The first few cells should be familiar to you."]},{"metadata":{"id":"Nva44BEYTfaM","colab_type":"code","colab":{}},"cell_type":"code","source":["#====================================================#\n","# This python notebook is created by Oscar PANG (oscarpang@vtc.edu.hk),\n","# Assistant Project Officer of VTC STEM Education Centre\n","# For distribution, please quote the author\n","#====================================================#\n","\n","import os, sys\n","import cv2\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import pyplot\n","from glob import glob"],"execution_count":0,"outputs":[]},{"metadata":{"id":"03W8ZMluY4Ks","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","!unzip /content/gdrive/My\\ Drive/public/plant*.zip "],"execution_count":0,"outputs":[]},{"metadata":{"id":"cIzgCE9zux-B","colab_type":"text"},"cell_type":"markdown","source":["## Upload dataset\n","\n","The dataset should be in the VM, as long as you have not reset the runtime or the 12-hour usage period is not yet lapsed. If not, load it again using the codes in the previous exercise."]},{"metadata":{"id":"cSw2XuwGwAWI","colab_type":"code","colab":{}},"cell_type":"code","source":["# Run the following command to browse the \"plant_seedlings_dataset\" directory. \n","# You should see two folders \"test\" and \"train\"\n","ls plant_seedlings_dataset"],"execution_count":0,"outputs":[]},{"metadata":{"id":"feAr7N_xwSBC","colab_type":"text"},"cell_type":"markdown","source":["## Define constants\n","\n","The input dimension required by VGG-16 is 224 x 224. Hence we have to resize the image data in order to fit the input requirement."]},{"metadata":{"id":"TayfD14HThRI","colab_type":"code","colab":{}},"cell_type":"code","source":["# define some constants and path names\n","label_path = \"plant_seedlings_dataset/train\"\n","train_path = \"plant_seedlings_dataset/train/*/*.png\"\n","test_path = \"plant_seedlings_dataset/test/*.png\"\n","\n","LENGTH = 224\n","IMG_SIZE = (LENGTH, LENGTH)\n","CHANNEL = 3"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YuH6g37fxVKH","colab_type":"text"},"cell_type":"markdown","source":["## Prepare and load the dataset\n","\n","First let's load the filenames of the plant seedling images and get the class names of the labels.\n","\n"]},{"metadata":{"id":"WgRvc0oSUZRa","colab_type":"code","colab":{}},"cell_type":"code","source":["# use glob to read the path of each image file\n","train_filenames = glob(train_path)\n","test_filenames = glob(test_path)\n","\n","#print(train_filenames)\n","#print(test_filenames)\n","\n","label_dict = {}\n","class_num = 0\n","\n","# create a dictionary for the key-value pairs of seed names and the respective values\n","for subdir in sorted(os.listdir(label_path)):\n","    label_dict[subdir] = class_num\n","    class_num+=1\n","\n","print(label_dict)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sV8MknwvBOLC","colab_type":"text"},"cell_type":"markdown","source":["Next, we randomize the order of the filename first."]},{"metadata":{"id":"Z48KVznkeVa5","colab_type":"code","colab":{}},"cell_type":"code","source":["train_filenames = np.asarray(train_filenames)\n","\n","np.random.seed(5)\n","indices = np.random.permutation(train_filenames.shape[0])\n","\n","train_filenames = train_filenames[indices]\n","\n","train_num = int(train_filenames.shape[0] * 0.8)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sYISnzprx_VE","colab_type":"text"},"cell_type":"markdown","source":["Then, we read the image data and the labels into the lists according to the randomized order of the filenames. This may take a while."]},{"metadata":{"id":"W7cE2qHlvWA0","colab_type":"code","colab":{}},"cell_type":"code","source":["x_Train = []\n","y_Train = []\n","x_Eval = []\n","y_Eval = []\n","\n","cnt = 0\n","\n","for filename in train_filenames:\n","    image = cv2.resize(cv2.imread(filename), IMG_SIZE)\n","    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n","    seed_name = filename.split(\"/\")[2]\n","    if cnt < train_num:\n","      x_Train.append(image)\n","      y_Train.append(label_dict[seed_name])\n","    else:\n","      x_Eval.append(image)\n","      y_Eval.append(label_dict[seed_name])\n","    cnt += 1\n","    \n","\n","# convert the lists into numpy array\n","x_Train = np.asarray(x_Train)\n","y_Train = np.asarray(y_Train)\n","x_Eval = np.asarray(x_Eval)\n","y_Eval = np.asarray(y_Eval)\n","\n","# examine the datasets\n","print(x_Train.shape)\n","print(y_Train.shape)\n","print(x_Eval.shape)\n","print(y_Eval.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lq3EwLwiyXrl","colab_type":"text"},"cell_type":"markdown","source":["We can visualize the image data read from the folders. (You can skip this part)"]},{"metadata":{"id":"PRDi90Auybf0","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.rcParams['figure.figsize'] = (10.0,8.0)\n","for i in range(20):\n","    plt.subplot(4,5,i+1)\n","    plt.imshow(x_Train[i])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"b-frOav4ylC5","colab_type":"text"},"cell_type":"markdown","source":["## Data Processing\n","\n","We don't do rescaling using numpy here as in the previous exercise. We will rescale it using ImageDataGenerator of Keras in the training runtime. This will save some resources in the VM."]},{"metadata":{"id":"hLzO1T1nyp18","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.utils import np_utils\n","\n","# perform one-hot encoding to the train labels\n","y_Train = np_utils.to_categorical(y_Train)\n","y_Eval = np_utils.to_categorical(y_Eval)\n","\n","print(y_Train[:20])\n","print(y_Eval[:20])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-lB2VPjmy_TC","colab_type":"text"},"cell_type":"markdown","source":["## Define helper functions"]},{"metadata":{"id":"gJ-yW2HAzBrn","colab_type":"code","colab":{}},"cell_type":"code","source":["# create helper function\n","def show_train_history(train_history, train, validation):\n","    plt.plot(train_history.history[train])\n","    plt.plot(train_history.history[validation])\n","    plt.title('Train History')\n","    plt.xlabel('Epochs')\n","    plt.ylabel(train)\n","    plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hEPxQde4zC0j","colab_type":"text"},"cell_type":"markdown","source":["## Build CNN Model"]},{"metadata":{"id":"gIjWKVYJzGD6","colab_type":"code","colab":{}},"cell_type":"code","source":["# network building..\n","# import packages\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Flatten, Dense, Dropout\n","from keras.optimizers import Adam, RMSprop\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications import VGG16\n","from keras.callbacks import TensorBoard"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E1nYFJ7SzLFq","colab_type":"text"},"cell_type":"markdown","source":["Next, import the pre-trained weights and architecture from VGG16 model. We freeze the weights of the convolution layers of the model and import it into our model."]},{"metadata":{"id":"rUjR7vivn1-p","colab_type":"code","colab":{}},"cell_type":"code","source":["vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(LENGTH, LENGTH, 3))\n","\n","# Freeze the layers except the last 4 layers\n","for layer in vgg_conv.layers[:-4]:\n","    layer.trainable = False\n","\n","# Check the trainable status of the individual layers\n","for layer in vgg_conv.layers:\n","    print(layer, layer.trainable)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"g6dJFQYon-0Q","colab_type":"code","colab":{}},"cell_type":"code","source":["vgg_conv.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"w6NCexLIgSoW","colab_type":"text"},"cell_type":"markdown","source":["We buld our CNN on top of the VGG-16 convolution layers."]},{"metadata":{"id":"sQvTuJL-zJRE","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","\n","# Create the model\n","model = Sequential()\n"," \n","# Add the vgg convolutional base model\n","model.add(vgg_conv)\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation = 'relu'))\n","model.add(Dropout(0.25))\n","model.add(Dense(512, activation = 'relu'))\n","model.add(Dropout(0.25))\n","#model.add(Dense(256, activation = 'relu'))\n","#model.add(Dropout(0.25))\n","model.add(Dense(12, activation = 'softmax'))\n","\n","model.summary()\n","\n","model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yIlXoT_fzbY9","colab_type":"text"},"cell_type":"markdown","source":["We perform rescaling of image data (0-255 to 0-1) using `ImageDataGenerator` of keras at runtime. This reduces memory consumption in the VM before training."]},{"metadata":{"id":"0qybcmI-oMN0","colab_type":"code","colab":{}},"cell_type":"code","source":["train_datagen = ImageDataGenerator(rescale=1./255)\n","test_datagen = ImageDataGenerator(rescale=1./255)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SDPFpJPlQGs7","colab_type":"text"},"cell_type":"markdown","source":["We use a smaller batch size (32) because of huge resource demand in VGG. Also, we use fit_generator for flowing train data and label from the `ImageDataGenerator`.\n","\n","We begin training with 10 epochs first. We can increase the epochs later."]},{"metadata":{"id":"4jgyEOVrzeHH","colab_type":"code","colab":{}},"cell_type":"code","source":["start_time = time.time()\n","#train_history = model.fit(x_Train, y_Train, epochs=40, batch_size=128, validation_data=(x_Eval, y_Eval))\n","\n","\n","train_history = model.fit_generator(train_datagen.flow(x_Train, y_Train, batch_size=32),\n","                    steps_per_epoch=len(x_Train) / 32, epochs=10, \n","                    validation_data=test_datagen.flow(x_Eval, y_Eval, batch_size=32),\n","                    validation_steps=20)\n","end_time = time.time()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qKbwvzgQ0UOS","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"Total time elapsed = %d sec\" % int(end_time - start_time))\n","print(\"=\"*70)\n","print()\n","# evaluate model accuracy\n","scores, accuracy = model.evaluate(x_Eval, y_Eval, batch_size=32)\n","print(\"model scores = \", scores)\n","print(\"model accuracy = \", accuracy)\n","\n","show_train_history(train_history, 'acc', 'val_acc')\n","#show_train_history(train_history, 'loss', 'val_loss')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sjhtNYWRU958","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"37U4-Bou9HWK","colab_type":"text"},"cell_type":"markdown","source":["With a simple CNN having 3 convolution layers, you have built a model achieving 70% of accuracy. This is not bad though! \n","\n","But this is not the end! We should strive for a better result. Now save the model for later use and move on to the next part of Lab 4!"]},{"metadata":{"id":"pd9kUheK7fvm","colab_type":"text"},"cell_type":"markdown","source":["## Save Trained Model"]},{"metadata":{"id":"RdN8nzIh7iVl","colab_type":"code","colab":{}},"cell_type":"code","source":["weight_file = str(time.strftime(\"%Y%m%d_%H%M%S\"))\n","weight_file = 'seedling_classifier_VGG_' + weight_file + '.h5'\n","model.save(weight_file)\n","print(\"Model weights have been saved successfully!\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5atgQobPfAUp","colab_type":"code","colab":{}},"cell_type":"code","source":["# serialize model to JSON\n","model_json = model.to_json()\n","model_name = 'model_vgg16_'+str(time.strftime(\"%Y%m%d_%H%M%S\"))+'.json'\n","with open(model_name, \"w\") as json_file:\n","    json_file.write(model_json)\n","\n","print(\"Saved model architecture to disk\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Ujlb7Dg3fjtg","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls\n","\n","# Download the saved model to your local system\n","from google.colab import files\n","files.download(filename)\n","files.download(model_name)"],"execution_count":0,"outputs":[]}]}